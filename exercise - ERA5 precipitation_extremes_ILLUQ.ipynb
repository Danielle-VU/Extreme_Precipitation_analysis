{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"3facdc23","cell_type":"code","source":"import xarray as xr\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport numpy as np\nimport scipy.stats as stats\nfrom scipy.stats import norm\nfrom scipy.stats import gamma\nfrom scipy.stats import genextreme as gev\n\nfrom pyextremes import EVA","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5d9faf1f","cell_type":"code","source":"# Load the ERA5 reanalysis daily precipitation data for different locations\nlocation = 'Ilulissat' # Inuvik, Longyearbyen, Deadhorse, Ilulisat\n\nfile_path = f'Data/ERA5_Arctic_TP_daysum_{location}_1940-2023.nc' # will change automatically once you adjusted the location\n\ndaily_precipitation = xr.open_dataset(file_path)\n# Add an attribute to the dataset\ndaily_precipitation.attrs['units'] = 'm/d' #you can check the units in the attributes listed in the table created by printing the loaded database\n# create a array which only conains the years \nyears = daily_precipitation['time'].dt.year.values\n\n# Print the loaded data base\ndaily_precipitation","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a7b76a79-0f01-4265-b963-f89f81c6242e","cell_type":"code","source":"# take the mean precipitation over the entire area\ndaily_precipitation = daily_precipitation.mean(dim=['latitude','longitude'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"35ddc537","cell_type":"code","source":"# Calculate monthly sums\nmonthly_sum_precipitation = daily_precipitation.resample(time='ME').sum(dim='time') # ME adjusted from 1m - deprication warning!\n# Calaculate a mean of monthly sums from the years\nannual_mean_precipitation_monthly_sum = monthly_sum_precipitation.groupby('time.month').mean(dim='time')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"193ad6d4","cell_type":"code","source":"# Convert DataArray to pandas DataFrame for easier plotting\ndf = annual_mean_precipitation_monthly_sum.to_dataframe()\n\n# Plot mean monthly precipitation of multi-annual means as a bar plot\ndf['tp'].plot(kind='bar', figsize=(10, 6))\nplt.title(f'{location} Spatial Mean of Multi-Annual Monthly Total Precipitation (Entire Domain)')\nplt.xlabel('Month')\nplt.ylabel('Total Precipitation [m]')\nplt.xticks(range(12), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], rotation=45)\nplt.grid(axis='y')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"da5fc66b","cell_type":"code","source":"# A diffrent way of ploting the precipitation allows us to learn something\n# about distribution of values within the months\n\n# First calaculate spatial mean for daily values which are grouped by month\nspatial_mean_daily_precipitation_monthly = daily_precipitation['tp'].groupby('time.month')\n\n# Prepare data for violin plots (daily values must be stored in lists for every month)\ndata_list = []\ndata_99p_list = []\nmonths = range(1, 13)  # Months range from 1 to 12\nfor m in months:\n    # Extract data for each month\n    data_month = spatial_mean_daily_precipitation_monthly[m]  \n    # Calaculate 99% percentile for each month\n    data_month_99p = data_month.reduce(np.percentile, q=99, dim='time')   \n    # store data into lists for each month \n    data_list.append(data_month.values)\n    data_99p_list.append(data_month_99p.values)\n    \n# Create violin plots we use the seaborn libary\nplt.figure(figsize=(10, 6))\nsns.violinplot(data=data_list, cut=0, color = 'lightblue')\nplt.title(f'{location} Multi-Annual Daily Precipitation Distribution per Month')\nplt.xlabel('Month')\nplt.ylabel('Mean daily precipitation [m]')\nplt.xticks(range(12), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\nplt.grid(axis='y')\n\nplt.plot(np.arange(0,12,1),data_99p_list, color='r', linestyle='--', label=f'99th Percentile')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"28817755","cell_type":"code","source":"# Optain array of precipitation data from the xarray\ndaily_mean = daily_precipitation['tp']\n\n# filter out only wet days (daily precipitation > 1mm --> BUT data in m \ndaily_mean_wet = daily_mean[(daily_mean > 0.001)]\ndaily_mean_dry = daily_mean[(daily_mean <= 0.001)]\n\nn_days = daily_mean.size\nn_wet_days = daily_mean_wet.size\nn_dry_days = daily_mean_dry.size\n\np_wet_day = n_wet_days/n_days\n\nprint(f'In total there are {n_wet_days:.0f} wet days out of {n_days:.0f} observations in {location}')\nprint(f'The mean daily propabilty of wet day is {p_wet_day:.2f} dry days out of {n_days:.0f} observations in {location}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b6940223","cell_type":"code","source":"# Calculate mean, median, and percentiles in mm\nmean_precip = daily_mean_wet.mean().values\nmedian_precip = np.nanmedian(daily_mean_wet.values)\npercentile_95 = np.nanpercentile(daily_mean_wet.values, 95)\npercentile_99 = np.nanpercentile(daily_mean_wet.values, 99)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8011fe80","cell_type":"code","source":"# Plot the histogram\nplt.figure(figsize=(8, 6))\nplt.hist(daily_mean_wet, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n\n# Add lines for mean, median, and percentiles\nplt.axvline(mean_precip, color='red', linestyle='dashed', linewidth=1, label=f'Mean: {mean_precip:.4f} m')\nplt.axvline(median_precip, color='green', linestyle='dashed', linewidth=1, label=f'Median: {median_precip:.4f} m')\nplt.axvline(percentile_95, color='orange', linestyle='dashed', linewidth=1, label=f'95th Percentile: {percentile_95:.4f} m')\nplt.axvline(percentile_99, color='purple', linestyle='dashed', linewidth=1, label=f'99th Percentile: {percentile_99:.4f} m')\n\n# Plot legend, labels, and title\nplt.legend()\nplt.xlabel('Daily Mean Precipitation (m)')\nplt.ylabel('Frequency')\nplt.title(f'Histogram of Daily Mean Precipitation with Percentiles for {location}')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2f36f6b4","cell_type":"code","source":"# Create a DataFrame for better organization and visualization\ndf = pd.DataFrame({'Date': daily_mean_wet['time'], 'DailyRecords': daily_mean_wet.values})\ndf","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"63e26c60","cell_type":"code","source":"# Now we use an approach from Extreme Value Analysis\n\n# Extract year from the 'Date' column\ndf['Year'] = df['Date'].dt.year\n\n# Find annual maxima and corresponding dates (Block Maximum)\nannual_maxima = df.loc[df.groupby('Year')['DailyRecords'].idxmax()]\n\n# Plot the time series\nplt.plot(df['Date'], df['DailyRecords'], label='Daily Records', linestyle='-', color='b', alpha=0.5)\n# Mark the annual maxima on the plot\nplt.scatter(annual_maxima['Date'], annual_maxima['DailyRecords'], color='red', marker='o', label='Annual Maxima')\nplt.title('Time Series with Annual Maxima Marked')\nplt.xlabel('Date')\nplt.ylabel('Max daily precipitation [m]')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"cf38d02c-566d-486a-93f1-87cc01a25549","cell_type":"markdown","source":"This dataframe stores the dates of all the 'wet' days (e.g. >1 mm of precipitation)\n\nThe following analysis is based on the block maximum method and calculates the return period for events > 30 mm. ","metadata":{}},{"id":"02bfd96a","cell_type":"code","source":"annual_maxima","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"bb51a3de","cell_type":"code","source":"# Now we fit the GEV function to the annual maxima\ngev_shape, gev_loc, gev_scale = gev.fit(annual_maxima['DailyRecords'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"cbb69d27","cell_type":"code","source":"# Create histogram\nplt.hist(annual_maxima['DailyRecords'], bins=30, density=True, alpha=0.6, color='g', edgecolor='black')\n\n# Plot the fitted function\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = gev(gev_shape,gev_loc,gev_scale).pdf(x)\nplt.plot(x, p, 'k', linewidth=2)\n\n# Add labels and title\nplt.title(f'Fit results: shape = {gev_shape:.2f}, loc = {gev_loc:.2f},  scale = {gev_scale:.2f}')\nplt.xlabel('Precipitation [m]')\nplt.ylabel('Frequency')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"63645e76","cell_type":"code","source":"event_threshold = 0.0179\n# Model the return period based on the GEV function usinfg the same threshold value  \np_annual_extreme = 1 - gev(gev_shape,gev_loc,gev_scale).cdf(event_threshold)\nreturn_period_extreme = 1/p_annual_extreme\nprint(return_period_extreme)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"50434075","cell_type":"markdown","source":" Perform an analysis for all the regions listed and think of a way to visualise/summarise this in a comprehensible way\n\n For this you need to think about a suitable way to show this (e.g. figures/tables etc)\n You also need to think about differences between the regions\n     - should the threshold be the same?\n     - what are the differences in precipitation patterns?\n\nTips:\nstart with comparing precipitation patterns","metadata":{}},{"id":"19bad246-cc46-480c-bb1a-f83512dd7759","cell_type":"code","source":"# Define locations to loop over\nlocations = ['Ilulissat', 'Inuvik', 'Longyearbyen', 'Deadhorse']\n\n# Initialize a figure with 2x2 subplots\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.flatten()\n\n# Loop through each location and create violin plots\nfor i, location in enumerate(locations):\n    file_path = f'Data/ERA5_Arctic_TP_daysum_{location}_1940-2023.nc'\n    \n    # Load dataset for the location\n    daily_precipitation = xr.open_dataset(file_path)\n    daily_precipitation = daily_precipitation.mean(dim=['latitude', 'longitude'])\n    \n    # Group daily precipitation by month\n    spatial_mean_daily_precipitation_monthly = daily_precipitation['tp'].groupby('time.month')\n    \n    # Prepare data for violin plots\n    data_list = []\n    data_99p_list = []\n    for m in range(1, 13):\n        data_month = spatial_mean_daily_precipitation_monthly[m]  \n        data_month_99p = data_month.reduce(np.percentile, q=99, dim='time')  \n        data_list.append(data_month.values)\n        data_99p_list.append(data_month_99p.values)\n    \n    # Create the violin plot for the current location\n    sns.violinplot(data=data_list, cut=0, color='lightblue', ax=axes[i])\n    axes[i].set_title(f'{location} Precipitation Distribution per Month')\n    axes[i].set_xlabel('Month')\n    axes[i].set_ylabel('Mean Daily Precipitation [m]')\n    axes[i].set_xticks(range(12))\n    axes[i].set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], rotation=45)\n    axes[i].grid(axis='y')\n\n    # Plot the 99th percentile line for the current location\n    axes[i].plot(np.arange(0, 12, 1), data_99p_list, color='r', linestyle='--', label=f'99th Percentile')\n    axes[i].legend()\n\n# Adjust layout and show the plot\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4fd014bd-dcd6-4d00-88e2-fa062a26d5e5","cell_type":"code","source":"# Initialize a list to store the results\nresults = []\n\n# Loop through each location\nfor location in locations:\n    file_path = f'Data/ERA5_Arctic_TP_daysum_{location}_1940-2023.nc'\n    \n    # Load dataset for the location\n    daily_precipitation = xr.open_dataset(file_path)\n    daily_precipitation = daily_precipitation.mean(dim=['latitude', 'longitude'])\n    \n    # Filter wet days (daily precipitation > 1mm)\n    daily_mean_wet = daily_precipitation['tp'][(daily_precipitation['tp'] > 0.001)]\n    \n    # Calculate the 99th percentile as the threshold for extreme events\n    threshold_99p = np.nanpercentile(daily_mean_wet.values, 99)\n    \n    # Convert dataset to pandas DataFrame for easier manipulation\n    df = pd.DataFrame({'Date': daily_mean_wet['time'].values, 'DailyRecords': daily_mean_wet.values})\n    \n    # Extract year from the 'Date' column\n    df['Year'] = df['Date'].dt.year\n    \n    # Find annual maxima using the block maxima approach\n    annual_maxima = df.loc[df.groupby('Year')['DailyRecords'].idxmax()]\n    \n    # Fit the GEV distribution to the annual maxima\n    gev_shape, gev_loc, gev_scale = gev.fit(annual_maxima['DailyRecords'])\n    \n    # Calculate the return period for the 99th percentile threshold\n    p_annual_extreme = 1 - gev.cdf(threshold_99p, gev_shape, gev_loc, gev_scale)\n    return_period_extreme = 1 / p_annual_extreme\n    \n    # Store the results in the list\n    results.append([location, threshold_99p, return_period_extreme])\n\n# Create a DataFrame from the results list\ndf_results = pd.DataFrame(results, columns=['Location', '99th Percentile Threshold (m)', 'Return Period (years)'])\n\n# Display the table\nprint(df_results)\n\n# Optional: You can also save this DataFrame to a CSV file if needed\n#df_results.to_csv('Data/return_periods_by_location.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"be3717c6-7495-4e75-8fd3-1ec022c9f6b7","cell_type":"code","source":"# Define a fixed threshold for all regions (e.g., 0.020m)\nfixed_threshold = 0.020\n\n# Initialize a list to store the results\nresults = []\n\n# Loop through each location\nfor location in locations:\n    file_path = f'Data/ERA5_Arctic_TP_daysum_{location}_1940-2023.nc'\n    \n    # Load dataset for the location\n    daily_precipitation = xr.open_dataset(file_path)\n    daily_precipitation = daily_precipitation.mean(dim=['latitude', 'longitude'])\n    \n    # Filter wet days (daily precipitation > 1mm)\n    daily_mean_wet = daily_precipitation['tp'][(daily_precipitation['tp'] > 0.001)]\n    \n    # Calculate the 99th percentile as the threshold for extreme events\n    threshold_99p = np.nanpercentile(daily_mean_wet.values, 99)\n    \n    # Convert dataset to pandas DataFrame for easier manipulation\n    df = pd.DataFrame({'Date': daily_mean_wet['time'].values, 'DailyRecords': daily_mean_wet.values})\n    \n    # Extract year from the 'Date' column\n    df['Year'] = df['Date'].dt.year\n    \n    # Find annual maxima using the block maxima approach\n    annual_maxima = df.loc[df.groupby('Year')['DailyRecords'].idxmax()]\n    \n    # Fit the GEV distribution to the annual maxima\n    gev_shape, gev_loc, gev_scale = gev.fit(annual_maxima['DailyRecords'])\n    \n    # Calculate the return period for the 99th percentile threshold\n    p_annual_extreme_99p = 1 - gev.cdf(threshold_99p, gev_shape, gev_loc, gev_scale)\n    return_period_extreme_99p = 1 / p_annual_extreme_99p\n    \n    # Calculate the return period for the fixed threshold (e.g., 0.020 m)\n    p_annual_extreme_fixed = 1 - gev.cdf(fixed_threshold, gev_shape, gev_loc, gev_scale)\n    return_period_extreme_fixed = 1 / p_annual_extreme_fixed\n    \n    # Find the maximum precipitation event from the data\n    max_precipitation = df['DailyRecords'].max()\n    \n    # Calculate the return period for the maximum precipitation event\n    p_annual_extreme_max = 1 - gev.cdf(max_precipitation, gev_shape, gev_loc, gev_scale)\n    return_period_extreme_max = 1 / p_annual_extreme_max\n    \n    # Store the results in the list\n    results.append([location, \n                    threshold_99p, return_period_extreme_99p, \n                    fixed_threshold, return_period_extreme_fixed, \n                    max_precipitation, return_period_extreme_max])\n\n# Create a DataFrame from the results list\ndf_results = pd.DataFrame(results, columns=[\n    'Location', \n    '99th %-tile Threshold (m)', 'Return Period (99th %-tile)', \n    'Fixed Threshold (m)', 'Return Period (Fixed Threshold)', \n    'Max Precipitation (m)', 'Return Period (Max Precipitation)'\n])\n\n# Display the table\nprint(df_results)\n\n# Optional: You can also save this DataFrame to a CSV file if needed\ndf_results.to_csv('return_periods_by_location_extended.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"29e47401-0c10-4e50-9c74-fc858e341842","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}